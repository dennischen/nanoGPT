{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fff4f8ed-ad88-40d2-9852-277872ebffb4",
   "metadata": {},
   "source": [
    "### A much shorter/ipynb version of train.py for demo sampling shakespeare_char\n",
    "\n",
    "This file is a refined version from sample.py with shakespeare-char and simpler/smaller config for quick demo integration in jupyter lab, it is the same as following command\n",
    "```\n",
    "python sample.py --out_dir=out-shakespeare-char --device=cpu --seed=9999 --num_samples=5\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c27bc77-8d08-4b62-b3f1-e60cd8ca7e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from contextlib import nullcontext\n",
    "import torch\n",
    "import tiktoken\n",
    "from model import GPTConfig, GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "687bc85a-1cbe-46d5-8ada-649f2535368b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "init_from = 'resume' # either 'resume' (from an out_dir) or a gpt2 variant (e.g. 'gpt2-xl')\n",
    "out_dir = 'out' # ignored if init_from is not 'resume'\n",
    "start = \"\\n\" # or \"<|endoftext|>\" or etc. Can also specify a file, use as: \"FILE:prompt.txt\"\n",
    "num_samples = 10 # number of samples to draw\n",
    "max_new_tokens = 500 # number of tokens generated in each sample\n",
    "temperature = 0.8 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
    "top_k = 200 # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
    "seed = 1337\n",
    "device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\n",
    "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32' or 'bfloat16' or 'float16'\n",
    "compile = False # use PyTorch 2.0 to compile the model to be faster\n",
    "# -----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30e6d248-b271-473a-b060-0c5e9afe93af",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "seed = 9999\n",
    "\n",
    "# shakespeare-char\n",
    "out_dir = 'out-shakespeare-char'\n",
    "num_samples = 5\n",
    "\n",
    "# gpt2\n",
    "# init_from = 'gpt2'\n",
    "# start='What is the answer to life, the universe, and everyting?'\n",
    "# num_samples = 3\n",
    "# max_new_tokens=100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b23f838c-e20e-41e1-9159-f9cdc14bae51",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cce5eea-0254-4a43-9a13-8c4a8e7e39eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 0.80M\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "if init_from == 'resume':\n",
    "    # init from a model saved in a specific directory\n",
    "    ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n",
    "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "    gptconf = GPTConfig(**checkpoint['model_args'])\n",
    "    model = GPT(gptconf)\n",
    "    state_dict = checkpoint['model']\n",
    "    unwanted_prefix = '_orig_mod.'\n",
    "    for k,v in list(state_dict.items()):\n",
    "        if k.startswith(unwanted_prefix):\n",
    "            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "    model.load_state_dict(state_dict)\n",
    "elif init_from.startswith('gpt2'):\n",
    "    # init from a given GPT-2 model\n",
    "    model = GPT.from_pretrained(init_from, dict(dropout=0.0))\n",
    "\n",
    "model.eval()\n",
    "model.to(device)\n",
    "if compile:\n",
    "    model = torch.compile(model) # requires PyTorch 2.0 (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e7f29b5-b3ab-4f1f-ac97-a4b39e64241b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading meta from data/shakespeare_char/meta.pkl...\n"
     ]
    }
   ],
   "source": [
    "# look for the meta pickle in case it is available in the dataset folder\n",
    "load_meta = False\n",
    "if init_from == 'resume' and 'config' in checkpoint and 'dataset' in checkpoint['config']: # older checkpoints might not have these...\n",
    "    meta_path = os.path.join('data', checkpoint['config']['dataset'], 'meta.pkl')\n",
    "    load_meta = os.path.exists(meta_path)\n",
    "if load_meta:\n",
    "    print(f\"Loading meta from {meta_path}...\")\n",
    "    with open(meta_path, 'rb') as f:\n",
    "        meta = pickle.load(f)\n",
    "    # TODO want to make this more general to arbitrary encoder/decoder schemes\n",
    "    stoi, itos = meta['stoi'], meta['itos']\n",
    "    encode = lambda s: [stoi[c] for c in s]\n",
    "    decode = lambda l: ''.join([itos[i] for i in l])\n",
    "else:\n",
    "    # ok let's assume gpt-2 encodings by default\n",
    "    print(\"No meta.pkl found, assuming GPT-2 encodings...\")\n",
    "    enc = tiktoken.get_encoding(\"gpt2\")\n",
    "    encode = lambda s: enc.encode(s, allowed_special={\"<|endoftext|>\"})\n",
    "    decode = lambda l: enc.decode(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0ef094d-5208-4b0e-a5bb-376b0e4f8f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the beginning of the prompt\n",
    "if start.startswith('FILE:'):\n",
    "    with open(start[5:], 'r', encoding='utf-8') as f:\n",
    "        start = f.read()\n",
    "start_ids = encode(start)\n",
    "x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9551c30-e13c-427a-948e-0ef4a3962712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The divat not the wiks foaker him? the\n",
      "Conercimest, o o I thad thaigh dind dible, oh'd you shat.\n",
      "\n",
      "\n",
      "CENELANTE:\n",
      "I shak fater thee shis shave dof sir mun o eveakd:\n",
      "I to that let iff kneact wer the to prasm is!\n",
      "\n",
      "RUCAMET:\n",
      "Wen soy ham ind the thae athe aste; and is,\n",
      "This oy is anld dron mompeseste in no b'st my on?\n",
      "\n",
      "\n",
      "Last, und shy and simas, him for twith beith woull chover.\n",
      "\n",
      "\n",
      "SILONENG:\n",
      "Ah, baw thy ame for a cand!\n",
      "\n",
      "WLUSOMER:\n",
      "ARou, hart mast eke nooof The this trashed, whe smeriads.\n",
      "\n",
      "\n",
      "Comaverte then li\n",
      "---------------\n",
      "\n",
      "Nut.\n",
      "\n",
      "HACABRIICEN:\n",
      "WhoulGou awstas forting sood marry heas doflle the tha we cone!\n",
      "we wous cord ofest prostarst, the of my I mere have the iploe\n",
      "thou so inceadlis'g ind if theight bust sang.\n",
      "\n",
      "INTAndur, kis and binow, and het thive nosus,\n",
      "A what sthat so the prays, in foughim hie shis and of.\n",
      "\n",
      "INILIUS:\n",
      "He wellt, fed eive, and, why dinge, thit you,\n",
      "Af thou thing apthe at. Duy I cheer,\n",
      "\n",
      "BEnnk in end fur and, my I ind the nonou thim sible\n",
      "And the lippporetit in wis a of for momodetn; wheres Withy hi\n",
      "---------------\n",
      "\n",
      "So tham of you's werod ougne cim.\n",
      "\n",
      "EDWINS:\n",
      "O would sall scord aford ond buth mile hat scourre sto with.\n",
      "\n",
      "\n",
      "LOFEONDE:\n",
      "O disho mat!\n",
      "\n",
      "The wn, thithy basters, for rof fie the wam yhe thous maghed.\n",
      "The conk we to the dof sit me tho she dong.\n",
      "I amy sebrnk not'd no:\n",
      "Ef sheserd by thehe be a willl ofor one's;\n",
      "You sill me a the A bee to fetf thee sheee,\n",
      "Ind mat and hiblle, hee ind endest willse!\n",
      "\n",
      "HAs shay tang tho us weren wir, be I lom op heer that.\n",
      "\n",
      "SeNGUKERD:\n",
      "Het, tas.\n",
      " my thou upprises\n",
      "And?\n",
      "\n",
      "\n",
      "TEMENRYT\n",
      "---------------\n",
      "\n",
      "this loven'd hou with corad a to wit torgustato,\n",
      "Ise ire dincts broloss ot by st prishe:\n",
      "That rof omhe off kyoou we merivers?\n",
      "\n",
      "OLERDWING:\n",
      "Wis thas of hought goos sheak sis st shaperen be.\n",
      "\n",
      "Agive thef 'I nopour sove abe we chakse steactir,\n",
      "Wis allf in aip isiend your sellte moged seaind thish\n",
      "Whis this rechers hom mathe so insing limble sing?\n",
      "\n",
      "The hit cost grabu:\n",
      "Bndinct. I fay I doll thou ee sof ther in no wit to ing's Reat.\n",
      "\n",
      "BULICES:\n",
      "Buth ubr a the sof too ploed with mes.\n",
      "\n",
      "\n",
      "SAMERLES:\n",
      "Hed:\n",
      "O tho\n",
      "---------------\n",
      "\n",
      "burk whe not lald simey, lourrs owngrer:\n",
      "A fost the hour now vemere thaper.\n",
      "\n",
      "Whan yOr thathau see tha lof corrdsoin a muctimes yoou:\n",
      "But wisec agh swilll I sra's lagiend malind\n",
      "Frans ir guin on:\n",
      "Wham, kis hy ney comf sorrd, the word to to me\n",
      "livan, 'the mashe: the the apptb anisst marrmarans.\n",
      "e Is sto my o the's, willl you in yemp,\n",
      "Leacceds themat sttreny shacemses and Ke whe and thair!\n",
      "\n",
      "BUMARIT:\n",
      "He him and as for nit Mally.\n",
      "\n",
      "\n",
      "CEOMLUY:\n",
      "I ren nof thou seay, be that thitine yould sasisis?\n",
      "\n",
      "\n",
      "WARCET\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "# run generation\n",
    "with torch.no_grad():\n",
    "    with ctx:\n",
    "        for k in range(num_samples):\n",
    "            y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
    "            print(decode(y[0].tolist()))\n",
    "            print('---------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MyEnv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
