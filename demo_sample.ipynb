{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fff4f8ed-ad88-40d2-9852-277872ebffb4",
   "metadata": {},
   "source": [
    "A much shorter/ipynb version of train.py for demo sampling shakespeare_char\n",
    "\n",
    "This file is a refined version from sample.py with shakespeare-char and simpler/smaller value config for quick demo integration in jupyter lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c27bc77-8d08-4b62-b3f1-e60cd8ca7e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from contextlib import nullcontext\n",
    "import torch\n",
    "import tiktoken\n",
    "from model import GPTConfig, GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "687bc85a-1cbe-46d5-8ada-649f2535368b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "init_from = 'resume' # either 'resume' (from an out_dir) or a gpt2 variant (e.g. 'gpt2-xl')\n",
    "out_dir = 'out' # ignored if init_from is not 'resume'\n",
    "start = \"\\n\" # or \"<|endoftext|>\" or etc. Can also specify a file, use as: \"FILE:prompt.txt\"\n",
    "num_samples = 10 # number of samples to draw\n",
    "max_new_tokens = 500 # number of tokens generated in each sample\n",
    "temperature = 0.8 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
    "top_k = 200 # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
    "seed = 1337\n",
    "device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\n",
    "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32' or 'bfloat16' or 'float16'\n",
    "compile = False # use PyTorch 2.0 to compile the model to be faster\n",
    "# -----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30e6d248-b271-473a-b060-0c5e9afe93af",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "\n",
    "# shakespeare-char\n",
    "out_dir = 'out-shakespeare-char'\n",
    "num_samples = 5\n",
    "\n",
    "# gpt2\n",
    "# init_from = 'gpt2'\n",
    "# start='What is the answer to life, the universe, and everyting?'\n",
    "# num_samples = 3\n",
    "# max_new_tokens=100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b23f838c-e20e-41e1-9159-f9cdc14bae51",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cce5eea-0254-4a43-9a13-8c4a8e7e39eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 0.80M\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "if init_from == 'resume':\n",
    "    # init from a model saved in a specific directory\n",
    "    ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n",
    "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "    gptconf = GPTConfig(**checkpoint['model_args'])\n",
    "    model = GPT(gptconf)\n",
    "    state_dict = checkpoint['model']\n",
    "    unwanted_prefix = '_orig_mod.'\n",
    "    for k,v in list(state_dict.items()):\n",
    "        if k.startswith(unwanted_prefix):\n",
    "            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "    model.load_state_dict(state_dict)\n",
    "elif init_from.startswith('gpt2'):\n",
    "    # init from a given GPT-2 model\n",
    "    model = GPT.from_pretrained(init_from, dict(dropout=0.0))\n",
    "\n",
    "model.eval()\n",
    "model.to(device)\n",
    "if compile:\n",
    "    model = torch.compile(model) # requires PyTorch 2.0 (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e7f29b5-b3ab-4f1f-ac97-a4b39e64241b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading meta from data/shakespeare_char/meta.pkl...\n"
     ]
    }
   ],
   "source": [
    "# look for the meta pickle in case it is available in the dataset folder\n",
    "load_meta = False\n",
    "if init_from == 'resume' and 'config' in checkpoint and 'dataset' in checkpoint['config']: # older checkpoints might not have these...\n",
    "    meta_path = os.path.join('data', checkpoint['config']['dataset'], 'meta.pkl')\n",
    "    load_meta = os.path.exists(meta_path)\n",
    "if load_meta:\n",
    "    print(f\"Loading meta from {meta_path}...\")\n",
    "    with open(meta_path, 'rb') as f:\n",
    "        meta = pickle.load(f)\n",
    "    # TODO want to make this more general to arbitrary encoder/decoder schemes\n",
    "    stoi, itos = meta['stoi'], meta['itos']\n",
    "    encode = lambda s: [stoi[c] for c in s]\n",
    "    decode = lambda l: ''.join([itos[i] for i in l])\n",
    "else:\n",
    "    # ok let's assume gpt-2 encodings by default\n",
    "    print(\"No meta.pkl found, assuming GPT-2 encodings...\")\n",
    "    enc = tiktoken.get_encoding(\"gpt2\")\n",
    "    encode = lambda s: enc.encode(s, allowed_special={\"<|endoftext|>\"})\n",
    "    decode = lambda l: enc.decode(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0ef094d-5208-4b0e-a5bb-376b0e4f8f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the beginning of the prompt\n",
    "if start.startswith('FILE:'):\n",
    "    with open(start[5:], 'r', encoding='utf-8') as f:\n",
    "        start = f.read()\n",
    "start_ids = encode(start)\n",
    "x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9551c30-e13c-427a-948e-0ef4a3962712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I by dou aw lalled to my'mbreng avak;\n",
      "This beeatle dare and sors blot beiong ind to him\n",
      "For ume thin afth, the my hasing,\n",
      "He with me whe wath noath will wourstas, murs's wor ith,\n",
      "Me mur iste more of bor me thee ashe it to on une,\n",
      "If ind praith a\n",
      "Yooust sho this lo ind pish's I pamem porep,\n",
      "I a shiin maker lin baland nown ite lis.\n",
      "\n",
      "\n",
      "LARDWUS:\n",
      "That cose sis tay, a and the be that me thee fall ave ay\n",
      "ince deanstss a baink arttint as thee his rally.\n",
      "\n",
      "INGCAUS:\n",
      "Ind Has's bof merie angllaing, a to thert\n",
      "---------------\n",
      "\n",
      "That row I son beod itre in me thip\n",
      "The pown hat cof gray anden. \n",
      "\n",
      "KENCUSTINCE:\n",
      "Noy pilen of swith some?\n",
      "\n",
      "\n",
      "THORK:\n",
      "Pull:\n",
      "I dof swanke meang, thou soard with, thee himts wist a gountue.\n",
      "\n",
      "H'Thir. Cowat with of marties:\n",
      "To be ha tis afy warst ris pen, so a be\n",
      "If wand sto be int omes bun a greanc?\n",
      "\n",
      "IBUchow hais I with ux-sche mamarrses of land the,\n",
      "What weith sfaid I wolld levet herge mastesst loled's\n",
      "dhat ind flest urreamterst som mirke nowng ther\n",
      "Welll oGf tham sout me wa thake for ble hish,\n",
      "'I tha\n",
      "---------------\n",
      "\n",
      "To lardee for fomspre ande, and youl thanged\n",
      "ath omand! in dot mint be bery, that surath st with;\n",
      "I wat Ind caish we your in a wor\n",
      "Furdir eshour pand not tho st astcells,\n",
      "An im thim hirt stut he he fort and beake aut with shen,\n",
      "This you prater vine the spiy ong in vent ther thish,\n",
      "You now shis this aat seye you knes anmes\n",
      "And this hour et shop ifoll yo thim insnkems o site nreans\n",
      "te theat the ont dor lome; flir and for a sustle\n",
      "Thard en then cung antier of and kianllour micht.\n",
      "\n",
      "BURICETVE:\n",
      "A'llw \n",
      "---------------\n",
      "\n",
      "LOUS:\n",
      "Trech, weildsing fily.\n",
      "\n",
      "Sirght hat the thaves ant, is a willl is dis is\n",
      "lim an for is of shicty for hibe.\n",
      "\n",
      "INut mugler st thip dee tis wshe wherep knoe\n",
      "Lard a'd bee grouiighth ked and frime\n",
      "Whath st be ditle ingse come win and your afetaion,\n",
      "And swart owith, willl lou kendice, my beng tee sheise,\n",
      "And aind for dof he as thich my your beast to a buse to an\n",
      "t for la gen ond wor the theat, wiche, worsse tit foe?\n",
      "And I whath bow I thou an werdce by's, with thim,\n",
      "Thais mous and ford My sisis a c\n",
      "---------------\n",
      "\n",
      "To this you rintlmang by the sthis\n",
      "Dure 'shack of for in sour lont your know\n",
      "And the hip him, agle tho boode sham the weathar.\n",
      "\n",
      "\n",
      "NGORYOVETO:\n",
      "Brentan:\n",
      "The the groaw'dath uwn but wit\n",
      "Of In thour nire aght, thy mo to is thick she weach is\n",
      "ot museds come ay lout the uis oft cord han and\n",
      "blpence the pamedoy; ave and quow, knot af af guere.\n",
      "\n",
      "ENGARUF:\n",
      "O you ou I my and, mut the thest I?\n",
      "Whar sha be wther wis this do thim, hom may.\n",
      "\n",
      "KIRG Yourd whas ou proink thie ind dexfuty ther\n",
      "st here somades, this i\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "# run generation\n",
    "with torch.no_grad():\n",
    "    with ctx:\n",
    "        for k in range(num_samples):\n",
    "            y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
    "            print(decode(y[0].tolist()))\n",
    "            print('---------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MyEnv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
